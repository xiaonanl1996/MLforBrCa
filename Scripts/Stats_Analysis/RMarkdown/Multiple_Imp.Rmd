---
title: "Multiple Imputation"
author: "Xiaonan Liu"
date: '2022-04-14'
output: html_document
---

```{r setup, include=FALSE,message=FALSE,warning=FALSE}
knitr::opts_chunk$set(include=TRUE, echo=FALSE)

library(kableExtra)
library(knitr)
library(yaml)
library(here)
library(ggplot2)
library(tidyr)
library(DiagrammeR)
library(DiagrammeRsvg)
library(magrittr)
library(svglite)
library(rsvg)
library(png)
library(grid)
library(dplyr)
library(survival)
library(survminer)
library(naniar)
library(summarytools)
library(rsample)      # data splitting 
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(lime)         # model visualization
library(vip)          # Feature importance visualisation
library(rms)
library(mice)
library(shiny)
library(gtools)
library(ggeffects)
library(mitml)
library(rcompanion)

# Specify the markdown format for knitr tables, otherwise they're not compatible with kableExtra
options(knitr.kable.NA='', knitr.table.format = "html")
options(kableExtra.auto_format = FALSE)
options(rstudio.connectionObserver.errorsSuppressed = TRUE)
# Set the root directory to the project directory (otherwise knitr works in the directory of the Rmd file by default)
knitr::opts_knit$set(root.dir = here::here())

# Load the project config file for filepaths etc
config = yaml.load_file(here::here("./config.yml"))
source(here::here(config$functions$JC))

#-------------------------------------------------------------------------------------------------------------------
# Setup

figure <- 1
table <- 1
suppfig <- 1
footnote_no <- 1


tot_train<-readRDS(here::here(file.path(config$data$derived,"tot_train.rds")))
tot_test<-readRDS(here::here(file.path(config$data$derived,"tot_test.rds")))

xgboost_SHAP<-read.csv(here::here(file.path(config$outputs$outputs,"XGBoost_SHAP.csv")))

```


```{r Data extraction,include=FALSE}

source(file.path(config$scripts$cleaning, "dataset_generator.R"))

pretty_names <- pretty_switch(field_definitions=c(TEU_SPECS$BrCa_PRS,TEU_SPECS$HES_CaR), return_type="list")
pretty_func <- pretty_switch(field_definitions=c(TEU_SPECS$BrCa_PRS,TEU_SPECS$HES_CaR), return_type = "function")

```

# Introduction

This report documents process of feature inspection (e.g. correlation elimination) and multiple imputation on training data and test data. Note: This Rmd is not for knitting but for running code chunks interactively.

# Features Inspection

First we would start with a set containing the union of the following:

* Top 20 'important' features based on SHAP values
* Other established breast cancer risk factors (BMI, physical activity, age at menarche, number of live births, hormone replacement treatment)

Note: The established breast cancer risk factors that were not picked out as top 20 important features from XGBoost all had non-zero SHAP values, they just didn't make to top 20.


```{r imp features}


imp_features<-c(
  xgboost_SHAP$Var_name[c(1:5,7:20)],
  # Because FaH of BrCa var name is in a different format
  "TEU_FaH_BrCa",
  # established RFs
  "BSM_BMI", "PhA_METsWkAllAct", "G03FA","FSF_PeriodAge",
   "FSF_NLiveBirths" 
)


```

Taking the union lead to total of `r length(imp_features)` features to start with. 

```{r}

sapply(imp_features,function(i) pretty_names[[i]])

```

# Correlation filtering (On Training Data)

Now that we've got the union of features, we performed a correlation based feature elimination (taking established RFs into account) using training data:  

For categorical vs categorical with a bias corrected Cramer's V, numeric vs numeric with Spearman (default), and categorical vs numeric with ANOVA. 

Threshold of 0.9 was used to identify sets of highly correlated predictors and removed all but one (the one with the least missingness unless very strong prior preference) to carry forward to cox model for further investigation.

This approach was based on [Madakkatel2021](https://www.nature.com/articles/s41598-021-02476-9).

```{r cor,cache=TRUE}

cor=mixed_assoc(tot_train%>%select(all_of(imp_features)))

# Filter for correlation above 0.9
cor%>%filter(x!=y & assoc>0.9)%>%
  rowwise()%>%
  mutate(x=pretty_func(x),
         y=pretty_func(y))%>%
  rename(Var1=x,Var2=y,Correlation=assoc)

```

The observation is variables from [Body composition by impedance](https://biobank.ndph.ox.ac.uk/showcase/label.cgi?id=100009) BMI and whole body fat mass are highly correlated (which is expected). We would remove BMI and retain whole body fat mass because whole body fat mass captures is assumed to be superior than BMI.

```{r missing train}

highcor=cor%>%filter(x!=y & assoc>0.9)%>%pull(x)%>%unique

tot_train%>%select(all_of(imp_features))%>%miss_var_summary()%>%rowwise()%>%mutate(variable=pretty_func(variable))

```


```{r remove Imp}

imp_features<-imp_features[!grepl("BSM_BMI",imp_features)]

```

Now we have total of `r length(imp_features)` features.

# Missingness (On Training Data)

After inspection of missingness above, **"Age at first birth"** should be treated as missing not at random. One possible way is to categorise them and treat the missingness as a category.

To retain information as much as possible, we kept as many categories as possible and also made sure the frequency of each category was too sparse. 


```{r categorise}

tot_train<-tot_train%>%
  mutate(TEU_FirstBirthAgeCat=factor(case_when(
    # If no live births given, assign "No births"
    FSF_NLiveBirths==0 ~ "No Births",
    FSF_NLiveBirths>0 & TEU_FirstBirthAge<20 ~ "<20",
    FSF_NLiveBirths>0 & TEU_FirstBirthAge>=20 & TEU_FirstBirthAge<30 ~ "20-30",
    FSF_NLiveBirths>0 & TEU_FirstBirthAge>=30 & TEU_FirstBirthAge<40 ~ "30-40",
    FSF_NLiveBirths>0 & TEU_FirstBirthAge>=40 ~ ">=40",
    TRUE ~ NA_character_
  ),levels=c("No Births","<20","20-30","30-40",">=40")),
  
  TEU_LastBirthAgeCat=factor(case_when(
    # If no live births given, assign "No births"
    FSF_NLiveBirths==0 ~ "No Births",
    FSF_NLiveBirths>0 & TEU_LastBirthAge<20 ~ "<20",
    FSF_NLiveBirths>0 & TEU_LastBirthAge>=20 & TEU_LastBirthAge<30 ~ "20-30",
    FSF_NLiveBirths>0 & TEU_LastBirthAge>=30 & TEU_LastBirthAge<40 ~ "30-40",
    FSF_NLiveBirths>0 & TEU_LastBirthAge>=40 ~ ">=40",
    TRUE ~ NA_character_
  ),levels=c("No Births","<20","20-30","30-40",">=40")),
  
  TEU_FatherDthAgeCat=factor(case_when(
    FaH_FatherAlive=="No" & FaH_FatherDeathAge<40 ~ "<40",
    FaH_FatherAlive=="No" & FaH_FatherDeathAge>=40 & FaH_FatherDeathAge<60 ~ "40-60",
    FaH_FatherAlive=="No" & FaH_FatherDeathAge>=60 & FaH_FatherDeathAge<80 ~ "60-80",
    FaH_FatherAlive=="No" & FaH_FatherDeathAge>=80 ~ ">=80",
    # If participants indicate their father still alive, assign alive
    FaH_FatherAlive=="Yes" ~ "Alive",
    TRUE ~ NA_character_
  ),levels = c("<40","40-60","60-80",">=80","Alive"))
  
  )

# Replace Father death age and age at first birth with the manipulated ones 

imp_features=replace(imp_features,imp_features=="TEU_FirstBirthAge","TEU_FirstBirthAgeCat")
#imp_features=replace(imp_features,imp_features=="TEU_LastBirthAge","TEU_LastBirthAgeCat")
#imp_features=replace(imp_features,imp_features=="FaH_FatherDeathAge","TEU_FatherDthAgeCat")

# Add those 2 vars pretty names 

pretty_names$TEU_FirstBirthAgeCat="Age at First birth (Categorical)"
#pretty_names$TEU_LastBirthAgeCat="Age at Last birth (Categorical)"
#pretty_names$TEU_FatherDthAgeCat="Father death age (Categorical)"

```

After data manipulation, check how much data we would lose by doing a complete case analysis:

```{r}

comp_data<-tot_train[,c("TEU_BrCa_status","TEU_BrCa_time",
                       imp_features,
                       "GeP_Array",paste0("GeP_PC_",1:10)
                       )]%>%na.omit

# How much data did we lose?
lose=round((nrow(tot_train)-nrow(comp_data))/nrow(tot_train)*100,2)

```
We would lose `r lose`% of the data due to missing data, hence we'd like to perform multiple imputation (MI).

# Multiple Imputation on training data


```{r MI data}

# Prepare MI data

MI_data=tot_train[,c("TEU_BrCa_status","TEU_BrCa_time",
                       imp_features,
                       "GeP_Array",paste0("GeP_PC_",1:10)
                       )]


# Need to Scale up PRS

MI_data=MI_data%>%
  mutate(TEU_BrCa_100k_PRS=TEU_BrCa_100k_PRS*115300*2,
         TEU_BrCa_313_PRS=TEU_BrCa_313_PRS*305*2)


```


```{r MI post-menopause,eval=FALSE}
# This process is time-consuming, hence eval=FALSE, I ran manually

# Create Imputations (iter=20, M=10)

# Using default imputation method based on variable type

# Since we have number of live births and age at first birth, we need to keep this relationship during MI
# ie. If women had no births, then age at first birth should be "No births"
ini <- mice(MI_data,maxit = 0,print=FALSE)
post <- ini$post
post["TEU_FirstBirthAgeCat"] <- "imp[[j]][data$FSF_NLiveBirths[!r[,j]]==0,i] <- 'No Births'"
post["FSF_NLiveBirths"] <- "imp[[j]][data$TEU_FirstBirthAgeCat[!r[,j]]=='No Births',i] <- 0"

pred <- quickpred(MI_data,include = c("TEU_BrCa_status", "TEU_BrCa_time",
                                      "TEU_BrCa_100k_PRS","TEU_BrCa_313_PRS",
                                          "TEU_BaC_AgeAtRec","Imp_BodyFatMass","PhA_METsWkAllAct",
                                          "TEU_Alc_WeeklyAlcUnits","TEU_FaH_BrCa",
                                          "G03FA","FSF_PeriodAge","FSF_NLiveBirths",
                                          "TEU_FirstBirthAgeCat"))

imp_post<-mice(MI_data,post=post,pred=pred,maxit = 20,seed = 100,m=10) 

save(imp_post,file=file.path(config$data$derived,'imp_post.rds'))



```

```{r test,eval=FALSE}

# Main purpose of this test is to make sure First Age birth and Number of live births are consistent throughout MI
test=mice(MI_data,maxit = 5,seed = 100,m=1,post=post)

test_long=complete(imp_post,"long")

test_long%>%filter(TEU_FirstBirthAgeCat!="No Births"&FSF_NLiveBirths==0)%>%select(TEU_FirstBirthAgeCat,FSF_NLiveBirths)

test_long%>%filter(TEU_LastBirthAgeCat!="No Births"&FSF_NLiveBirths==0)%>%select(TEU_LastBirthAgeCat,FSF_NLiveBirths)

```

```{r Diagnostics,eval=FALSE}

## 1. Assess Convergence 
plot(imp_post)

## 2. Compare summary stats between observed and completed (observed + imputed) data

# Extract completed data after MI

imp1=imp_post

#MI_data=MI_data_post;imp1=imp_post

  
long1 <- complete(imp1,"long") 
long1$.imp <- as.factor(long1$.imp)



num_plot <- list() #Save the density plots
factor_tb <- list() # Save the freq tables 

# Variables with missing data
missing_var=MI_data%>%miss_var_summary()%>%filter(n_miss!=0)%>%pull(variable)

for (var in missing_var){
  
  if (is.numeric(MI_data[[var]])){
    # If numeric, plot density between observed and imputed 
    
    #long1 <- cbind(long1, ind.na=is.na(imp1$data[[var]]))
    long1 <- long1 %>%
      mutate(ind.na=rep(is.na(imp1$data[[var]]),10))
    
    p<-densityplot(~long1[[var]]|.imp, data=long1, group=ind.na, plot.points=FALSE,
                ref=TRUE, xlab=paste0(var),scales=list(y=list(draw=F)),
                par.settings=simpleTheme(col.line=rep(c("blue","red"))), 
                auto.key = list(columns=2,text=c("Observed","Imputed")))
    
    num_plot[[var]]=p
    
  }else{
    # If factor, produce freq table of each level between observed and imputed 
    long1 <- long1 %>%
      mutate(ind.na=rep(ifelse(is.na(imp1$data[[var]]),'Imputed','Observed'),10))
    
    factor_tb[[var]] <- lapply(unique(long1$.imp), function(i) 
      prop.table(table(long1%>%filter(.imp==i)%>%pull(ind.na),
                       long1%>%filter(.imp==i)%>%pull(var)), margin = 1)*100)
    
  }
}


# For continuous variables that had missing <10%, we probably shouldn't be counting on density plot much

lessmiss_var=MI_data%>%miss_var_summary()%>%filter(pct_miss<10 & pct_miss!=0 & !variable %in% names(factor_tb))%>%pull(variable)

MIcompare<-function(variable){
  
  print(variable)
  
  if(is.numeric(MI_data[[variable]])){
    # print observed summary
  print(summary(MI_data[[variable]]))
  
  # print observed + imputed summary
  print(lapply(1:10, function(i) summary(long1%>%filter(.imp==i)%>%pull(!!variable))))
  
  }else{
    # print observed freq
  print(prop.table(table(MI_data[[variable]])))
  
  # print observed + imputed freq
  print(lapply(1:10, function(i) prop.table(table(long1%>%filter(.imp==i)%>%pull(!!variable)))))
    
  }

}

MIcompare(lessmiss_var[1])

MIcompare(names(factor_tb)[1])


```

```{r post-processing,eval=FALSE}

# Scale alcohol per week to per day
# Scale PRS to percentiles
# Age into 3 groups for later inspection 

long1<-complete(imp_post,"long",include=TRUE)

comp_long1<-long1%>%
  mutate(
    TEU_Alc_DailyAlcUnits=TEU_Alc_WeeklyAlcUnits/7,
    TEU_BaC_AgeCat=cut(TEU_BaC_AgeAtRec,
                       breaks = c(40, 53, 60, 70),
                       labels = c("[40,53)", "[53,60)", "[60,70)"),
                       right = FALSE
    ))%>%
  group_by(.imp)%>%
  mutate(TEU_BrCa_100k_PRS_percent=Percentile(TEU_BrCa_100k_PRS),
         TEU_BrCa_313_PRS_percent=Percentile(TEU_BrCa_313_PRS),
         TEU_BrCa_100k_PRS_quint=quantcut(TEU_BrCa_100k_PRS,q=5,labels = c('Q1: Lowest score','Q2','Q3','Q4','Q5: Highest score')),
         TEU_BrCa_313_PRS_quint=quantcut(TEU_BrCa_313_PRS,q=5,labels = c('Q1: Lowest score','Q2','Q3','Q4','Q5: Highest score')))
  

comp_long1<-as.mids(comp_long1)


save(comp_long1,file=file.path(config$data$derived,'imp_post_derived.rds'))


```

```{r post-processing scale,eval=FALSE}

# Post-hoc: Scale variables with HR CI (1,1) to show wider CI in forest plot
# Scale alcohol per week to per day
# Create quintiles for PRS

long1<-complete(imp_post,"long",include=TRUE)

comp_long1_scaled<-long1%>%
  mutate(
    TEU_Alc_DailyAlcUnits=TEU_Alc_WeeklyAlcUnits/7,
    )%>%
  group_by(.imp)%>%
  mutate(
    # Scale variables by mean and sd within each imp
    Imp_MetRate=scale_this(Imp_MetRate),
    Uri_Sodium=scale_this(Uri_Sodium),
    Uri_Creat=scale_this(Uri_Creat),
    BBC_ALP_Result=scale_this(BBC_ALP_Result),
    PhA_METsWkAllAct=scale_this(PhA_METsWkAllAct),
    TEU_BrCa_100k_PRS_quint=quantcut(TEU_BrCa_100k_PRS,q=5,labels = c('Q1: Lowest score','Q2','Q3','Q4','Q5: Highest score')),
    TEU_BrCa_313_PRS_quint=quantcut(TEU_BrCa_313_PRS,q=5,labels = c('Q1: Lowest score','Q2','Q3','Q4','Q5: Highest score'))
  )
  

comp_long1_scaled<-as.mids(comp_long1_scaled)


save(comp_long1_scaled,file=file.path(config$data$derived,'imp_post_derived_scaled.rds'))


```


# Multiple Imputation on test data

```{r}


imp_features<-c(
  xgboost_SHAP$Var_name[c(1:5,7:20)],
  # Because FaH of BrCa var name is in a different format
  "TEU_FaH_BrCa",
  # established RFs
  "BSM_BMI", "PhA_METsWkAllAct", "G03FA","FSF_PeriodAge",
  "FSF_NLiveBirths" 
)


# Remove whole body fat mass due to high cor with BMI
imp_features<-imp_features[!grepl("BSM_BMI",imp_features)]


# Categorise first birth age 
tot_test<-tot_test%>%
  mutate(TEU_FirstBirthAgeCat=factor(case_when(
    # If no live births given, assign "No births"
    FSF_NLiveBirths==0 ~ "No Births",
    FSF_NLiveBirths>0 & TEU_FirstBirthAge<20 ~ "<20",
    FSF_NLiveBirths>0 & TEU_FirstBirthAge>=20 & TEU_FirstBirthAge<30 ~ "20-30",
    FSF_NLiveBirths>0 & TEU_FirstBirthAge>=30 & TEU_FirstBirthAge<40 ~ "30-40",
    FSF_NLiveBirths>0 & TEU_FirstBirthAge>=40 ~ ">=40",
    TRUE ~ NA_character_
  ),levels=c("No Births","<20","20-30","30-40",">=40"))
  
  )

# Replace Father death age and age at first birth with the manipulated ones 

imp_features=replace(imp_features,imp_features=="TEU_FirstBirthAge","TEU_FirstBirthAgeCat")

# Check missingness
tot_test%>%select(all_of(imp_features))%>%miss_var_summary()


```


```{r prep}

# Prepare MI data

MI_data=tot_test[,c("TEU_BrCa_status","TEU_BrCa_time",
                     imp_features,
                     "GeP_Array",paste0("GeP_PC_",1:10)
)]


# Need to Scale up PRS

MI_data=MI_data%>%
  mutate(TEU_BrCa_100k_PRS=TEU_BrCa_100k_PRS*115300*2,
         TEU_BrCa_313_PRS=TEU_BrCa_313_PRS*305*2)

```

```{r MI test}
ini <- mice(MI_data,maxit = 0,print=FALSE)
post <- ini$post
post["TEU_FirstBirthAgeCat"] <- "imp[[j]][data$FSF_NLiveBirths[!r[,j]]==0,i] <- 'No Births'"
post["FSF_NLiveBirths"] <- "imp[[j]][data$TEU_FirstBirthAgeCat[!r[,j]]=='No Births',i] <- 0"

pred <- quickpred(MI_data,include = c("TEU_BrCa_status", "TEU_BrCa_time",
                                      "TEU_BrCa_100k_PRS","TEU_BrCa_313_PRS",
                                      "TEU_BaC_AgeAtRec","Imp_BodyFatMass","PhA_METsWkAllAct",
                                      "TEU_Alc_WeeklyAlcUnits","TEU_FaH_BrCa",
                                      "G03FA","FSF_PeriodAge","FSF_NLiveBirths",
                                      "TEU_FirstBirthAgeCat"))

imp_post_test<-mice(MI_data,post=post,pred=pred,maxit = 20,seed = 100,m=10) 

save(imp_post_test,file=file.path(config$data$derived,'imp_post_test.rds'))

```

```{r}
plot(imp_post_test)

# Extract completed data after MI

imp1=imp_post_test

#MI_data=MI_data_post;imp1=imp_post


long1 <- complete(imp1,"long") 
long1$.imp <- as.factor(long1$.imp)



num_plot <- list() #Save the density plots
factor_tb <- list() # Save the freq tables 

# Variables with missing data
missing_var=MI_data%>%miss_var_summary()%>%filter(n_miss!=0)%>%pull(variable)

for (var in missing_var){
  
  if (is.numeric(MI_data[[var]])){
    # If numeric, plot density between observed and imputed 
    
    #long1 <- cbind(long1, ind.na=is.na(imp1$data[[var]]))
    long1 <- long1 %>%
      mutate(ind.na=rep(is.na(imp1$data[[var]]),10))
    
    p<-densityplot(~long1[[var]]|.imp, data=long1, group=ind.na, plot.points=FALSE,
                   ref=TRUE, xlab=paste0(var),scales=list(y=list(draw=F)),
                   par.settings=simpleTheme(col.line=rep(c("blue","red"))), 
                   auto.key = list(columns=2,text=c("Observed","Imputed")))
    
    num_plot[[var]]=p
    
  }else{
    # If factor, produce freq table of each level between observed and imputed 
    long1 <- long1 %>%
      mutate(ind.na=rep(ifelse(is.na(imp1$data[[var]]),'Imputed','Observed'),10))
    
    factor_tb[[var]] <- lapply(unique(long1$.imp), function(i) 
      prop.table(table(long1%>%filter(.imp==i)%>%pull(ind.na),
                       long1%>%filter(.imp==i)%>%pull(var)), margin = 1)*100)
    
  }
}

```


```{r}
# Scale alcohol per week to per day
# Scale PRS to percentiles
# Age into 3 groups for later inspection 

long1<-complete(imp_post_test,"long",include=TRUE)

comp_long1<-long1%>%
  mutate(
    TEU_Alc_DailyAlcUnits=TEU_Alc_WeeklyAlcUnits/7,
    TEU_BaC_AgeCat=cut(TEU_BaC_AgeAtRec,
                       breaks = c(40, 53, 60, 70),
                       labels = c("[40,53)", "[53,60)", "[60,70)"),
                       right = FALSE
    ))%>%
  group_by(.imp)%>%
  mutate(TEU_BrCa_100k_PRS_percent=Percentile(TEU_BrCa_100k_PRS),
         TEU_BrCa_313_PRS_percent=Percentile(TEU_BrCa_313_PRS),
         TEU_BrCa_100k_PRS_quint=quantcut(TEU_BrCa_100k_PRS,q=5,labels = c('Q1: Lowest score','Q2','Q3','Q4','Q5: Highest score')),
         TEU_BrCa_313_PRS_quint=quantcut(TEU_BrCa_313_PRS,q=5,labels = c('Q1: Lowest score','Q2','Q3','Q4','Q5: Highest score')))


comp_long1_test<-as.mids(comp_long1)


save(comp_long1_test,file=file.path(config$data$derived,'imp_post_test_derived.rds'))

```













